{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a2460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "import os\n",
    "import pickle\n",
    "from bs4 import Comment\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58afe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class html_check():\n",
    "    def __init__(self,text,url):  ##input the html and the domain name for example\n",
    "                                     \n",
    "        self.text = text\n",
    "        self.url = url\n",
    "        self.domain = tldextract.extract(self.url).domain\n",
    "        \n",
    "#------------------------------length of HTML----------------------------------------------      \n",
    "    \n",
    "    def find_len(self,tag):\n",
    "        if tag =='!--':\n",
    "            soup = self.text\n",
    "            len_sum = 0\n",
    "            for comment in soup.find_all(string=lambda text:isinstance(text, Comment)):\n",
    "                len_sum += len(comment)\n",
    "            return len_sum          \n",
    "        else:\n",
    "            soup = self.text\n",
    "            scripts = soup.find_all(str(tag))\n",
    "            len_sum = 0\n",
    "\n",
    "            for script in scripts:\n",
    "                  len_sum += len(script.text)\n",
    "            return len_sum\n",
    "        \n",
    "    def len_html_tag(self):  ## this is the total length for 5 special tags\n",
    "        return html_check.find_len(self,\"style\") + html_check.find_len(self,\"link\") + html_check.find_len(self,\"form\") + html_check.find_len(self,\"!--\") + html_check.find_len(self,\"script\")\n",
    "\n",
    "    def len_html(self):\n",
    "        return len(self.text.text)\n",
    "    \n",
    "#------------------------------hidden content------------------------------------------------------------------------\n",
    "    def hidden_div(self):\n",
    "        soup = self.text\n",
    "        scripts = soup.find_all('div')          \n",
    "        find = 0\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                if str(script.attrs['style'])=='visibility:hidden' or str(script.attrs['style'])=='display:none':\n",
    "                    find = 1    \n",
    "                    break               \n",
    "            except:\n",
    "                continue\n",
    "        return find \n",
    "    \n",
    "    def hidden_button(self):\n",
    "        soup = self.text\n",
    "        scripts = soup.find_all('button')          \n",
    "        find = 0\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                if str(script.attrs['disabled'])=='disabled':\n",
    "                    find = 1   \n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "        return find \n",
    "    \n",
    "    def hidden_input(self):\n",
    "        soup = self.text\n",
    "        scripts = soup.find_all('input')          \n",
    "        find = 0\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                if str(script.attrs['type'])=='hidden' or str(script.attrs['disabled'])=='disabled':\n",
    "                    find = 1\n",
    "                    break\n",
    "            except:\n",
    "                continue                \n",
    "        return find \n",
    "    \n",
    "  \n",
    "    def hidden(self): ## have hidden content\n",
    "        return int(html_check.hidden_div(self) | html_check.hidden_button(self) | html_check.hidden_input(self))\n",
    "    \n",
    "#----------------------------link based------------------------------------------------    \n",
    "\n",
    "    def find_all_link(self):\n",
    "        soup = self.text\n",
    "        a_tags = soup.find_all('a')\n",
    "        a_data = []\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            try:\n",
    "                a_data.append(a_tag.attrs['href'])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return a_data\n",
    "    \n",
    "    \n",
    "    def find_source(self,tag):  ## find src attribute in <img> <link> ... \n",
    "        if tag =='link':\n",
    "            soup = self.text\n",
    "            links = soup.find_all('link')\n",
    "            link_data = []\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    link_data.append(link.attrs['href'])\n",
    "                except:\n",
    "                    continue\n",
    "            return link_data\n",
    "        \n",
    "        else:\n",
    "            soup = self.text\n",
    "            resources = soup.find_all(str(tag))\n",
    "            data = []\n",
    "\n",
    "            for resource in resources:\n",
    "                try:\n",
    "                    data.append(resource.attrs['src'])       \n",
    "                except:\n",
    "                    continue\n",
    "            return data\n",
    "        \n",
    "    def internal_external_link(self):  ## Number of internal hyperlinks and number of external hyperlinks\n",
    "        link_list = html_check.find_all_link(self)\n",
    "        if len(link_list)==0:  ## in case there is no hyperlink\n",
    "            return [0, 0]  \n",
    "        \n",
    "        count = 0\n",
    "        for j in link_list:\n",
    "            if \"http\" in j:\n",
    "                brand = tldextract.extract(j).domain\n",
    "                if str(brand) == self.domain:\n",
    "                    count += 1\n",
    "            else:\n",
    "                count +=1                \n",
    "       \n",
    "        return [count, len(link_list) - count]\n",
    "\n",
    "    def empty_link(self): ## Number of empty links\n",
    "        link_list = html_check.find_all_link(self)\n",
    "        count = 0\n",
    "        for j in link_list:\n",
    "            if j==\"\" or j==\"#\" or j=='#javascript::void(0)' or j=='#content' or j=='#skip' or j=='javascript:;' or j=='javascript::void(0);' or j=='javascript::void(0)':\n",
    "                count += 1\n",
    "        if len(link_list)==0:\n",
    "            return 0\n",
    "        return count \n",
    "\n",
    "#----------------------------form based---------------------------------------------------    \n",
    "    \n",
    "    def find_form(self):\n",
    "        soup = self.text\n",
    "        forms = soup.find_all('form')\n",
    "        data = []\n",
    "\n",
    "        for form in forms:\n",
    "            input_tags = form.find_all('input')\n",
    "            for input_ in input_tags:\n",
    "                try:\n",
    "                    if input_.has_key('name'):\n",
    "                        data.append(str(input_['name']))\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def login_form(self): ## have login-form requires password\n",
    "        input_list = html_check.find_form(self)\n",
    "        result = 0\n",
    "        for j in input_list:\n",
    "            if j.find(\"password\")!=-1 or j.find(\"pass\")!=-1 or j.find(\"login\")!=-1 or j.find(\"signin\")!=-1:\n",
    "                result = 1\n",
    "                break\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def internal_external_resource(self): ##\n",
    "        tag_list = ['link','img','script','noscript']\n",
    "        resource_list = []\n",
    "        count = 0\n",
    "        for tag in tag_list:\n",
    "            resource_list.append(html_check.find_source(self,tag))\n",
    "        \n",
    "        resource_list = [y for x in resource_list for y in x]\n",
    "        if len(resource_list)==0: ## in case there is no resource link\n",
    "            return [0, 0]\n",
    "\n",
    "        for j in resource_list:\n",
    "            if \"http\" in j:\n",
    "                if not(self.domain == tldextract.extract(j).domain):\n",
    "                    count +=1   \n",
    "        \n",
    "        return len(resource_list) - count, count\n",
    "    \n",
    "    \n",
    "#-----------------suspicious element HTML-------------------------------------------------        \n",
    "     \n",
    "    def redirect(self): ##auto-refresh webpage\n",
    "        soup = self.text\n",
    "        return int('redirect' in soup)   \n",
    "                                  \n",
    "    def alarm_window(self):  ## alert window pop up\n",
    "        soup = self.text\n",
    "        scripts = soup.find_all('script')          \n",
    "        find = 0\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                if('alert' in str(script.contents)) or ('window.open' in str(script.contents)):\n",
    "                    find = 1\n",
    "                    break  \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return find\n",
    "#---------------------------------domain vs HTML content----------------------------------------\n",
    "    def title_domain(self):\n",
    "        soup = self.text\n",
    "        try:\n",
    "            return int(self.domain.lower() in soup.title.text.lower())\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def domain_occurrence(self):\n",
    "        try:\n",
    "            return str(self.text).count(self.domain)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def brand_freq_domain(self):\n",
    "        link_list = html_check.find_all_link(self)\n",
    "        domain_list = []\n",
    "        \n",
    "        for j in link_list:\n",
    "            if \"http\" in j:\n",
    "                brand = tldextract.extract(j).domain\n",
    "                domain_list.append(brand)\n",
    "            else:\n",
    "                domain_list.append(self.domain)\n",
    "              \n",
    "        if len(domain_list) == 0:\n",
    "            return 1\n",
    "        if pd.Series(domain_list).value_counts().index[0] == self.domain:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "265662e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class URL_check():\n",
    "    \n",
    "    def __init__(self, url, tldlist_path):\n",
    "        self.url = url.lower()\n",
    "        self.tldlist_path = tldlist_path\n",
    "    def domain_is_IP(self):\n",
    "        \n",
    "        if len(tldextract.extract(self.url).subdomain) == 0:\n",
    "            hostname = tldextract.extract(self.url).domain\n",
    "        else:\n",
    "            hostname = '.'.join([tldextract.extract(self.url).subdomain, tldextract.extract(self.url).domain])\n",
    "          \n",
    "        if np.sum([i.isdigit() for i in hostname.split(\".\")])==4:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0   \n",
    "\n",
    "    def symbol_count(self):\n",
    "        \n",
    "        punctuation_list = [\"@\", '-' ,'~']\n",
    "        count=0\n",
    "        for j in punctuation_list:\n",
    "            if j in self.url:\n",
    "                count+=1\n",
    "        return count\n",
    "\n",
    "    def https(self):\n",
    "        return int(\"https://\" in self.url)\n",
    "    \n",
    "    def domain_len(self):\n",
    "        if len(tldextract.extract(self.url).subdomain)==0:\n",
    "            domain_len = len(tldextract.extract(self.url).domain+\".\"+tldextract.extract(self.url).suffix)\n",
    "        else:\n",
    "            domain_len=len(tldextract.extract(self.url).subdomain +\".\"+ tldextract.extract(self.url).domain+\".\"+tldextract.extract(self.url).suffix)\n",
    "        return domain_len\n",
    "    \n",
    "    def url_len(self):\n",
    "        return len(self.url)\n",
    "\n",
    "    def num_dot_hostname(self):\n",
    "        if len(tldextract.extract(self.url).subdomain) == 0:\n",
    "            hostname = tldextract.extract(self.url).domain\n",
    "        else:\n",
    "            hostname = '.'.join([tldextract.extract(self.url).subdomain, tldextract.extract(self.url).domain])\n",
    "        return hostname.count('.')\n",
    "    \n",
    "    def sensitive_word(self):\n",
    "        sensitive_list = [\"secure\", \"account\", \"webscr\", \"login\",\n",
    "                          \"signin\", \"ebayisapi\", \"banking\", \"confirm\"]\n",
    "        return int(any(x in self.url for x in sensitive_list))\n",
    " \n",
    "    def tld_in_domain(self):\n",
    "        path = self.tldlist_path\n",
    "        tld_list = list(pd.read_csv(path, encoding = \"ISO-8859-1\").Domain.apply(lambda x: x.replace('.','')))\n",
    "        \n",
    "        for tld in tld_list: \n",
    "            if tld == tldextract.extract(self.url).subdomain or tld == tldextract.extract(self.url).domain:\n",
    "                return 1\n",
    "        return 0\n",
    "        \n",
    "    def tld_in_path(self):\n",
    "        path = self.tldlist_path\n",
    "        tld_list = list(pd.read_csv(path, encoding = \"ISO-8859-1\").Domain.apply(lambda x: x.replace('.','')))\n",
    "        \n",
    "        for tld in tld_list:\n",
    "            if tld == urlparse(self.url).path or tld == urlparse(self.url).params or tld == urlparse(self.url).query or tld == urlparse(self.url).fragment:\n",
    "                return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf902bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading h5py\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Set default parameters\n",
    "defaults = {\n",
    "    'output_basedir': 'TestResults',\n",
    "    'modeldir': 'model_directory',\n",
    "    'tldlist': 'tld.csv'\n",
    "}\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 500\n",
    "\n",
    "print('loading h5py')\n",
    "\n",
    "# Extract HTML features\n",
    "internal_link = []\n",
    "external_link = []\n",
    "empty_link = []\n",
    "login_form = []\n",
    "html_len_tag = []\n",
    "html_len = []\n",
    "alarm_window = []\n",
    "redirection = []\n",
    "hidden = []\n",
    "title_domain = []\n",
    "brand_domain = []\n",
    "internal_resource = []\n",
    "external_resource = []\n",
    "domain_occurrence = []\n",
    "\n",
    "# Extract URL features\n",
    "domain_is_ip = []\n",
    "symbol_count = []\n",
    "http = []\n",
    "domain_len = []\n",
    "url_len = []\n",
    "num_dot_hostname = []\n",
    "sensitive_word = []\n",
    "tld_in_domain = []\n",
    "tld_in_path = []\n",
    "\n",
    "phish = []\n",
    "\n",
    "# Load data in batches\n",
    "with h5py.File('../data/phishing_output.h5', 'r') as h5_file:\n",
    "    # Pull 'urls' and 'labels' from the 'test' dataset\n",
    "    total_entries = len(h5_file['train/urls'])\n",
    "    \n",
    "    count = 0\n",
    "    for start in range(0, total_entries, batch_size):\n",
    "        count += 1\n",
    "        print(count)\n",
    "        end = min(start + batch_size, total_entries)\n",
    "        urls_batch = [url.decode('utf-8') for url in h5_file['train/urls'][start:end]]\n",
    "        raw_html_content_batch = h5_file['train/html_content'][start:end]\n",
    "        labels_batch = h5_file['train/labels'][start:end]\n",
    "\n",
    "        # Process HTML content in the current batch\n",
    "        content = []\n",
    "        for i in range(len(raw_html_content_batch)):\n",
    "            try:\n",
    "                with open(raw_html_content_batch[i]) as html_content:\n",
    "                    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "                    if len(soup) == 0:\n",
    "                        content.append('')\n",
    "                    else:\n",
    "                        content.append(soup)\n",
    "            except Exception as e:\n",
    "                content.append('')\n",
    "\n",
    "\n",
    "        for i in range(len(content)):\n",
    "            phish.append(labels_batch[i])\n",
    "            if len(content[i]) == 0:\n",
    "                internal_link.append(0)\n",
    "                external_link.append(0)\n",
    "                empty_link.append(0)\n",
    "                login_form.append(0)\n",
    "\n",
    "                html_len_tag.append(0)\n",
    "                html_len.append(0)\n",
    "\n",
    "                alarm_window.append(0)\n",
    "                redirection.append(0)\n",
    "                hidden.append(0)\n",
    "\n",
    "                title_domain.append(0)\n",
    "                internal_resource.append(0)\n",
    "                external_resource.append(0)\n",
    "                domain_occurrence.append(0)\n",
    "                brand_domain.append(0)\n",
    "            else:\n",
    "                test = html_check(content[i], urls_batch[i])\n",
    "                internal_link.append(test.internal_external_link()[0])\n",
    "                external_link.append(test.internal_external_link()[1])\n",
    "                empty_link.append(test.empty_link())\n",
    "                login_form.append(test.login_form())\n",
    "\n",
    "                html_len_tag.append(test.len_html_tag())\n",
    "                html_len.append(test.len_html())\n",
    "\n",
    "                alarm_window.append(test.alarm_window())\n",
    "                redirection.append(test.redirect())\n",
    "                hidden.append(test.hidden())\n",
    "\n",
    "                title_domain.append(test.title_domain())\n",
    "                internal_resource.append(test.internal_external_resource()[0])\n",
    "                external_resource.append(test.internal_external_resource()[1])\n",
    "                domain_occurrence.append(test.domain_occurrence())\n",
    "                brand_domain.append(test.brand_freq_domain())\n",
    "\n",
    "        for i in range(len(urls_batch)):\n",
    "            url_class = URL_check(urls_batch[i], defaults['tldlist'])\n",
    "            domain_is_ip.append(url_class.domain_is_IP())\n",
    "            symbol_count.append(url_class.symbol_count())\n",
    "            http.append(url_class.https())\n",
    "            domain_len.append(url_class.domain_len())\n",
    "            url_len.append(url_class.url_len())\n",
    "            num_dot_hostname.append(url_class.num_dot_hostname())\n",
    "            sensitive_word.append(url_class.sensitive_word())\n",
    "            tld_in_domain.append(url_class.tld_in_domain())\n",
    "            tld_in_path.append(url_class.tld_in_path())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89119168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56635\n",
      "21296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['my-model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine features into a dataframe\n",
    "import xgboost as xgb\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import KFold,train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error,accuracy_score\n",
    "from sklearn.datasets import load_iris,load_digits,load_boston\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_feature = pd.concat([\n",
    "    pd.Series(internal_link), pd.Series(external_link), pd.Series(empty_link),\n",
    "    pd.Series(login_form), pd.Series(html_len_tag), pd.Series(html_len),\n",
    "    pd.Series(alarm_window), pd.Series(redirection), pd.Series(hidden),\n",
    "    pd.Series(title_domain), pd.Series(brand_domain), pd.Series(internal_resource),\n",
    "    pd.Series(external_resource), pd.Series(domain_occurrence), pd.Series(domain_is_ip),\n",
    "    pd.Series(symbol_count), pd.Series(http), pd.Series(domain_len),\n",
    "    pd.Series(url_len), pd.Series(num_dot_hostname), pd.Series(sensitive_word),\n",
    "    pd.Series(tld_in_domain), pd.Series(tld_in_path), pd.Series(phish)\n",
    "], axis=1)\n",
    "\n",
    "df_feature.columns = ['internal_link', 'external_link', 'empty_link',\n",
    "                      'login_form', 'html_len_tag', 'html_len',\n",
    "                      'alarm_window', 'redirection', 'hidden',\n",
    "                      'title_domain', 'brand_domain', 'internal_resource', 'external_resource',\n",
    "                      'domain_occurrence', 'domain_is_ip', 'symbol_count', 'http',\n",
    "                      'domain_len', 'url_len', 'num_dot_hostname', 'sensitive_word',\n",
    "                      'tld_in_domain', 'tld_in_path', 'phish']\n",
    "\n",
    "y_train = df_feature['phish']\n",
    "x_train = df_feature.drop(columns=['phish'])\n",
    "\n",
    "\n",
    "# Training XGBoost model\n",
    "print(len(x_train))\n",
    "print(list(y_train).count(1))\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(x_train, y_train)\n",
    "joblib.dump(xgb_model, 'my-model.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a758bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading h5py\n",
      "Batch 0: True Positives: 0, True Negatives: 92, False Positives: 8, False Negatives: 0\n",
      "Number of predicted positives in batch 0: 8\n",
      "Batch 100: True Positives: 0, True Negatives: 175, False Positives: 25, False Negatives: 0\n",
      "Number of predicted positives in batch 100: 17\n",
      "Batch 200: True Positives: 0, True Negatives: 264, False Positives: 36, False Negatives: 0\n",
      "Number of predicted positives in batch 200: 11\n",
      "Batch 300: True Positives: 0, True Negatives: 349, False Positives: 51, False Negatives: 0\n",
      "Number of predicted positives in batch 300: 15\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Set default parameters\n",
    "defaults = {\n",
    "    'output_basedir': 'TestResults',\n",
    "    'modeldir': 'model_directory',\n",
    "    'tldlist': 'tld.csv'\n",
    "}\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize counters for metrics\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "def update_metrics(y_true, y_pred):\n",
    "    global true_positives, true_negatives, false_positives, false_negatives\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            true_positives += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            true_negatives += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            false_positives += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "            \n",
    "print('loading h5py')\n",
    "\n",
    "# Load data in batches\n",
    "with h5py.File('../data/phishing_output.h5', 'r') as h5_file:\n",
    "    # Pull 'urls' and 'labels' from the 'test' dataset\n",
    "    total_entries = len(h5_file['producthunt/urls'])\n",
    "    \n",
    "    for start in range(0, total_entries, batch_size):\n",
    "        end = min(start + batch_size, total_entries)\n",
    "        urls_batch = [url.decode('utf-8') for url in h5_file['producthunt/urls'][start:end]]\n",
    "        raw_html_content_batch = h5_file['producthunt/html_content'][start:end]\n",
    "        labels_batch = h5_file['producthunt/labels'][start:end]\n",
    "\n",
    "        # Process HTML content in the current batch\n",
    "        content = []\n",
    "        for i in range(len(raw_html_content_batch)):\n",
    "            try:\n",
    "                with open(raw_html_content_batch[i]) as html_content:\n",
    "                    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "                    if len(soup) == 0:\n",
    "                        content.append('')\n",
    "                    else:\n",
    "                        content.append(soup)\n",
    "            except Exception as e:\n",
    "                content.append('')\n",
    "\n",
    "        # Extract HTML features\n",
    "        internal_link = []\n",
    "        external_link = []\n",
    "        empty_link = []\n",
    "        login_form = []\n",
    "        html_len_tag = []\n",
    "        html_len = []\n",
    "        alarm_window = []\n",
    "        redirection = []\n",
    "        hidden = []\n",
    "        title_domain = []\n",
    "        brand_domain = []\n",
    "        internal_resource = []\n",
    "        external_resource = []\n",
    "        domain_occurrence = []\n",
    "\n",
    "        for i in range(len(content)):\n",
    "            if len(content[i]) == 0:\n",
    "                internal_link.append(0)\n",
    "                external_link.append(0)\n",
    "                empty_link.append(0)\n",
    "                login_form.append(0)\n",
    "\n",
    "                html_len_tag.append(0)\n",
    "                html_len.append(0)\n",
    "\n",
    "                alarm_window.append(0)\n",
    "                redirection.append(0)\n",
    "                hidden.append(0)\n",
    "\n",
    "                title_domain.append(0)\n",
    "                internal_resource.append(0)\n",
    "                external_resource.append(0)\n",
    "                domain_occurrence.append(0)\n",
    "                brand_domain.append(0)\n",
    "            else:\n",
    "                test = html_check(content[i], urls_batch[i])\n",
    "                internal_link.append(test.internal_external_link()[0])\n",
    "                external_link.append(test.internal_external_link()[1])\n",
    "                empty_link.append(test.empty_link())\n",
    "                login_form.append(test.login_form())\n",
    "\n",
    "                html_len_tag.append(test.len_html_tag())\n",
    "                html_len.append(test.len_html())\n",
    "\n",
    "                alarm_window.append(test.alarm_window())\n",
    "                redirection.append(test.redirect())\n",
    "                hidden.append(test.hidden())\n",
    "\n",
    "                title_domain.append(test.title_domain())\n",
    "                internal_resource.append(test.internal_external_resource()[0])\n",
    "                external_resource.append(test.internal_external_resource()[1])\n",
    "                domain_occurrence.append(test.domain_occurrence())\n",
    "                brand_domain.append(test.brand_freq_domain())\n",
    "\n",
    "        # Extract URL features\n",
    "        domain_is_ip = []\n",
    "        symbol_count = []\n",
    "        http = []\n",
    "        domain_len = []\n",
    "        url_len = []\n",
    "        num_dot_hostname = []\n",
    "        sensitive_word = []\n",
    "        tld_in_domain = []\n",
    "        tld_in_path = []\n",
    "\n",
    "        for i in range(len(urls_batch)):\n",
    "            url_class = URL_check(urls_batch[i], defaults['tldlist'])\n",
    "            domain_is_ip.append(url_class.domain_is_IP())\n",
    "            symbol_count.append(url_class.symbol_count())\n",
    "            http.append(url_class.https())\n",
    "            domain_len.append(url_class.domain_len())\n",
    "            url_len.append(url_class.url_len())\n",
    "            num_dot_hostname.append(url_class.num_dot_hostname())\n",
    "            sensitive_word.append(url_class.sensitive_word())\n",
    "            tld_in_domain.append(url_class.tld_in_domain())\n",
    "            tld_in_path.append(url_class.tld_in_path())\n",
    "\n",
    "        # Combine features into a dataframe\n",
    "        df_feature = pd.concat([\n",
    "            pd.Series(internal_link), pd.Series(external_link), pd.Series(empty_link),\n",
    "            pd.Series(login_form), pd.Series(html_len_tag), pd.Series(html_len),\n",
    "            pd.Series(alarm_window), pd.Series(redirection), pd.Series(hidden),\n",
    "            pd.Series(title_domain), pd.Series(brand_domain), pd.Series(internal_resource),\n",
    "            pd.Series(external_resource), pd.Series(domain_occurrence), pd.Series(domain_is_ip),\n",
    "            pd.Series(symbol_count), pd.Series(http), pd.Series(domain_len),\n",
    "            pd.Series(url_len), pd.Series(num_dot_hostname), pd.Series(sensitive_word),\n",
    "            pd.Series(tld_in_domain), pd.Series(tld_in_path)\n",
    "        ], axis=1)\n",
    "\n",
    "        df_feature.columns = ['internal_link', 'external_link', 'empty_link',\n",
    "                              'login_form', 'html_len_tag', 'html_len',\n",
    "                              'alarm_window', 'redirection', 'hidden',\n",
    "                              'title_domain', 'brand_domain', 'internal_resource', 'external_resource',\n",
    "                              'domain_occurrence', 'domain_is_ip', 'symbol_count', 'http',\n",
    "                              'domain_len', 'url_len', 'num_dot_hostname', 'sensitive_word',\n",
    "                              'tld_in_domain', 'tld_in_path']\n",
    "\n",
    "        df_feature.index = urls_batch\n",
    "        df_feature.to_csv(os.path.join(defaults['output_basedir'], f'feature_batch_{start}.csv'))\n",
    "\n",
    "        # Get predictions\n",
    "        x = df_feature\n",
    "\n",
    "        y_prob = xgb_model.predict_proba(x)[:, 1]\n",
    "        y_pred = xgb_model.predict(x)\n",
    "\n",
    "        # Update metrics\n",
    "        update_metrics(labels_batch, y_pred)\n",
    "\n",
    "        # Output metrics for the current batch\n",
    "        print(\"Batch {}: True Positives: {}, True Negatives: {}, False Positives: {}, False Negatives: {}\"\n",
    "              .format(start, true_positives, true_negatives, false_positives, false_negatives))\n",
    "\n",
    "        pred_df = pd.DataFrame({'URL': urls_batch, 'y_pred': y_pred, 'y_prob': y_prob})\n",
    "        print(\"Number of predicted positives in batch {}: {}\".format(start, list(pred_df['y_pred']).count(1)))\n",
    "        pred_df.to_csv(os.path.join(defaults['output_basedir'], f'predict_batch_{start}.csv'), index=None)\n",
    "\n",
    "\n",
    "# Create the predicted and actual labels based on the counts\n",
    "y_true = [1] * true_positives + [0] * false_negatives + [1] * false_negatives + [0] * true_negatives\n",
    "y_pred = [1] * (true_positives + false_positives) + [0] * (false_negatives + true_negatives)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Output the stats\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce277277",
   "metadata": {},
   "source": [
    "With Pretrained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b33fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Set default parameters\n",
    "defaults = {\n",
    "    'output_basedir': 'TestResults',\n",
    "    'modeldir': 'model_directory',\n",
    "    'tldlist': 'tld.csv'\n",
    "}\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Initialize counters for metrics\n",
    "true_positives = 0\n",
    "true_negatives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "def update_metrics(y_true, y_pred):\n",
    "    global true_positives, true_negatives, false_positives, false_negatives\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            true_positives += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            true_negatives += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            false_positives += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            false_negatives += 1\n",
    "\n",
    "with open(os.path.join('../data/stack-model.pkl'), 'rb') as handle:\n",
    "    model = pickle.load(handle)\n",
    "        \n",
    "            \n",
    "print('loading h5py')\n",
    "\n",
    "# Load data in batches\n",
    "with h5py.File('../data/phishing_output.h5', 'r') as h5_file:\n",
    "    # Pull 'urls' and 'labels' from the 'test' dataset\n",
    "    total_entries = len(h5_file['test/urls'])\n",
    "    \n",
    "    for start in range(0, total_entries, batch_size):\n",
    "        end = min(start + batch_size, total_entries)\n",
    "        urls_batch = [url.decode('utf-8') for url in h5_file['producthunt/urls'][start:end]]\n",
    "        raw_html_content_batch = h5_file['producthunt/html_content'][start:end]\n",
    "        labels_batch = h5_file['producthunt/labels'][start:end]\n",
    "\n",
    "        # Process HTML content in the current batch\n",
    "        content = []\n",
    "        for i in range(len(raw_html_content_batch)):\n",
    "            try:\n",
    "                with open(raw_html_content_batch[i]) as html_content:\n",
    "                    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "                    if len(soup) == 0:\n",
    "                        content.append('')\n",
    "                    else:\n",
    "                        content.append(soup)\n",
    "            except Exception as e:\n",
    "                content.append('')\n",
    "\n",
    "        # Extract HTML features\n",
    "        internal_link = []\n",
    "        external_link = []\n",
    "        empty_link = []\n",
    "        login_form = []\n",
    "        html_len_tag = []\n",
    "        html_len = []\n",
    "        alarm_window = []\n",
    "        redirection = []\n",
    "        hidden = []\n",
    "        title_domain = []\n",
    "        brand_domain = []\n",
    "        internal_resource = []\n",
    "        external_resource = []\n",
    "        domain_occurrence = []\n",
    "\n",
    "        for i in range(len(content)):\n",
    "            if len(content[i]) == 0:\n",
    "                internal_link.append(0)\n",
    "                external_link.append(0)\n",
    "                empty_link.append(0)\n",
    "                login_form.append(0)\n",
    "\n",
    "                html_len_tag.append(0)\n",
    "                html_len.append(0)\n",
    "\n",
    "                alarm_window.append(0)\n",
    "                redirection.append(0)\n",
    "                hidden.append(0)\n",
    "\n",
    "                title_domain.append(0)\n",
    "                internal_resource.append(0)\n",
    "                external_resource.append(0)\n",
    "                domain_occurrence.append(0)\n",
    "                brand_domain.append(0)\n",
    "            else:\n",
    "                test = html_check(content[i], urls_batch[i])\n",
    "                internal_link.append(test.internal_external_link()[0])\n",
    "                external_link.append(test.internal_external_link()[1])\n",
    "                empty_link.append(test.empty_link())\n",
    "                login_form.append(test.login_form())\n",
    "\n",
    "                html_len_tag.append(test.len_html_tag())\n",
    "                html_len.append(test.len_html())\n",
    "\n",
    "                alarm_window.append(test.alarm_window())\n",
    "                redirection.append(test.redirect())\n",
    "                hidden.append(test.hidden())\n",
    "\n",
    "                title_domain.append(test.title_domain())\n",
    "                internal_resource.append(test.internal_external_resource()[0])\n",
    "                external_resource.append(test.internal_external_resource()[1])\n",
    "                domain_occurrence.append(test.domain_occurrence())\n",
    "                brand_domain.append(test.brand_freq_domain())\n",
    "\n",
    "        # Extract URL features\n",
    "        domain_is_ip = []\n",
    "        symbol_count = []\n",
    "        http = []\n",
    "        domain_len = []\n",
    "        url_len = []\n",
    "        num_dot_hostname = []\n",
    "        sensitive_word = []\n",
    "        tld_in_domain = []\n",
    "        tld_in_path = []\n",
    "\n",
    "        for i in range(len(urls_batch)):\n",
    "            url_class = URL_check(urls_batch[i], defaults['tldlist'])\n",
    "            domain_is_ip.append(url_class.domain_is_IP())\n",
    "            symbol_count.append(url_class.symbol_count())\n",
    "            http.append(url_class.https())\n",
    "            domain_len.append(url_class.domain_len())\n",
    "            url_len.append(url_class.url_len())\n",
    "            num_dot_hostname.append(url_class.num_dot_hostname())\n",
    "            sensitive_word.append(url_class.sensitive_word())\n",
    "            tld_in_domain.append(url_class.tld_in_domain())\n",
    "            tld_in_path.append(url_class.tld_in_path())\n",
    "\n",
    "        # Combine features into a dataframe\n",
    "        df_feature = pd.concat([\n",
    "            pd.Series(internal_link), pd.Series(external_link), pd.Series(empty_link),\n",
    "            pd.Series(login_form), pd.Series(html_len_tag), pd.Series(html_len),\n",
    "            pd.Series(alarm_window), pd.Series(redirection), pd.Series(hidden),\n",
    "            pd.Series(title_domain), pd.Series(brand_domain), pd.Series(internal_resource),\n",
    "            pd.Series(external_resource), pd.Series(domain_occurrence), pd.Series(domain_is_ip),\n",
    "            pd.Series(symbol_count), pd.Series(http), pd.Series(domain_len),\n",
    "            pd.Series(url_len), pd.Series(num_dot_hostname), pd.Series(sensitive_word),\n",
    "            pd.Series(tld_in_domain), pd.Series(tld_in_path)\n",
    "        ], axis=1)\n",
    "\n",
    "        df_feature.columns = ['internal_link', 'external_link', 'empty_link',\n",
    "                              'login_form', 'html_len_tag', 'html_len',\n",
    "                              'alarm_window', 'redirection', 'hidden',\n",
    "                              'title_domain', 'brand_domain', 'internal_resource', 'external_resource',\n",
    "                              'domain_occurrence', 'domain_is_ip', 'symbol_count', 'http',\n",
    "                              'domain_len', 'url_len', 'num_dot_hostname', 'sensitive_word',\n",
    "                              'tld_in_domain', 'tld_in_path']\n",
    "\n",
    "        df_feature.index = urls_batch\n",
    "        df_feature.to_csv(os.path.join(defaults['output_basedir'], f'feature_batch_{start}.csv'))\n",
    "\n",
    "        # Get predictions\n",
    "        x = df_feature\n",
    "\n",
    "        y_prob = model.predict_proba(x)[:, 1]\n",
    "        y_pred = model.predict(x)\n",
    "\n",
    "        # Update metrics\n",
    "        update_metrics(labels_batch, y_pred)\n",
    "\n",
    "        # Output metrics for the current batch\n",
    "        print(\"Batch {}: True Positives: {}, True Negatives: {}, False Positives: {}, False Negatives: {}\"\n",
    "              .format(start, true_positives, true_negatives, false_positives, false_negatives))\n",
    "\n",
    "        pred_df = pd.DataFrame({'URL': urls_batch, 'y_pred': y_pred, 'y_prob': y_prob})\n",
    "        print(\"Number of predicted positives in batch {}: {}\".format(start, list(pred_df['y_pred']).count(1)))\n",
    "        pred_df.to_csv(os.path.join(defaults['output_basedir'], f'predict_batch_{start}.csv'), index=None)\n",
    "\n",
    "\n",
    "# Create the predicted and actual labels based on the counts\n",
    "y_true = [1] * true_positives + [0] * false_negatives + [1] * false_negatives + [0] * true_negatives\n",
    "y_pred = [1] * (true_positives + false_positives) + [0] * (false_negatives + true_negatives)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Output the stats\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f09fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_phish-base",
   "language": "python",
   "name": "conda_phish-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
