{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343df2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00e0786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tf1.13/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished build vocabulary and mapping to x in 0.9714772701263428\n",
      "Size of word vocabulary: 5792\n",
      "Number of words with freq >= 1: 5792\n",
      "Finished build vocabulary and mapping to x in 0.810955286026001\n",
      "Size of word vocabulary: 73498\n",
      "Processing #url 0\n",
      "Size of ngram vocabulary: 82\n",
      "Size of word vocabulary: 5588\n",
      "Index of <UNKNOWN> word: 1182\n",
      "Overall Mal/Ben split: 23963/39798\n",
      "Train Mal/Ben split: 21567/35819\n",
      "Test Mal/Ben split: 2396/3979\n",
      "Train/Test split: 57386/6375\n",
      "Train/Test split: 57386/6375\n",
      "Writing to runs/10000/\n",
      "\n",
      "Number of batches in total: 2245\n",
      "Number of batches per epoch: 449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emb_mode 5 delimit_mode 1 train_size 57386: 100% 2245/2245 [27:30<00:00,  1.36it/s, dev_acc=9.515e-01, dev_loss=1.410e-01, min_dev_loss=1.410e-01, trn_acc=9.844e-01, trn_loss=7.134e-02]  \n"
     ]
    }
   ],
   "source": [
    "# This is a Jupyter Notebook version of the given TensorFlow 1.13 training script\n",
    "\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bisect import bisect_left\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from TextCNN import *\n",
    "from utils import *\n",
    "\n",
    "# Configuration dictionary to replace argparse parameters\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"max_len_words\": 200,\n",
    "        \"max_len_chars\": 200,\n",
    "        \"max_len_subwords\": 20,\n",
    "        \"min_word_freq\": 1,\n",
    "        \"dev_pct\": 0.1,\n",
    "        \"data_dir\": 'train_10000.txt',\n",
    "        \"delimit_mode\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"emb_dim\": 32,\n",
    "        \"filter_sizes\": \"3,4,5,6\",\n",
    "        \"emb_mode\": 5\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"nb_epochs\": 5,\n",
    "        \"batch_size\": 128,\n",
    "        \"l2_reg_lambda\": 0.0,\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"log\": {\n",
    "        \"output_dir\": \"runs/10000/\",\n",
    "        \"print_every\": 50,\n",
    "        \"eval_every\": 500,\n",
    "        \"checkpoint_every\": 500\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print configuration settings\n",
    "for section, params in config.items():\n",
    "    for key, value in params.items():\n",
    "        print(f\"{section}.{key}={value}\")\n",
    "\n",
    "# Load data\n",
    "with h5py.File('../data/phishing_output.h5', 'r') as h5_file:\n",
    "    # Pull 'urls' and 'labels' from the 'train' dataset\n",
    "    train_urls = [url.decode('utf-8') for url in h5_file['train/urls'][:]]\n",
    "    train_labels = h5_file['train/labels'][:]\n",
    "\n",
    "    # Pull 'urls' and 'labels' from the 'dev' dataset\n",
    "    dev_urls = [url.decode('utf-8') for url in h5_file['dev/urls'][:]]\n",
    "    dev_labels = h5_file['dev/labels'][:]\n",
    "\n",
    "    # Concatenate the 'urls' and 'labels'\n",
    "    urls = train_urls + dev_urls\n",
    "    labels = np.concatenate((train_labels, dev_labels))\n",
    "\n",
    "high_freq_words = None\n",
    "if config[\"data\"][\"min_word_freq\"] > 0:\n",
    "    x1, word_reverse_dict = get_word_vocab(urls, config[\"data\"][\"max_len_words\"], config[\"data\"][\"min_word_freq\"])\n",
    "    high_freq_words = sorted(list(word_reverse_dict.values()))\n",
    "    print(\"Number of words with freq >= {}: {}\".format(config[\"data\"][\"min_word_freq\"], len(high_freq_words)))\n",
    "\n",
    "x, word_reverse_dict = get_word_vocab(urls, config[\"data\"][\"max_len_words\"])\n",
    "word_x = get_words(x, word_reverse_dict, config[\"data\"][\"delimit_mode\"], urls)\n",
    "ngramed_id_x, ngrams_dict, worded_id_x, words_dict = ngram_id_x(word_x, config[\"data\"][\"max_len_subwords\"], high_freq_words)\n",
    "\n",
    "chars_dict = ngrams_dict\n",
    "chared_id_x = char_id_x(urls, chars_dict, config[\"data\"][\"max_len_chars\"])\n",
    "\n",
    "# Split data into positive and negative samples\n",
    "pos_x = []\n",
    "neg_x = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    if label == 1:\n",
    "        pos_x.append(i)\n",
    "    else:\n",
    "        neg_x.append(i)\n",
    "print(\"Overall Mal/Ben split: {}/{}\".format(len(pos_x), len(neg_x)))\n",
    "pos_x = np.array(pos_x)\n",
    "neg_x = np.array(neg_x)\n",
    "\n",
    "x_train, y_train, x_test, y_test = prep_train_test(pos_x, neg_x, config[\"data\"][\"dev_pct\"])\n",
    "\n",
    "x_train_char = get_ngramed_id_x(x_train, ngramed_id_x)\n",
    "x_test_char = get_ngramed_id_x(x_test, ngramed_id_x)\n",
    "\n",
    "x_train_word = get_ngramed_id_x(x_train, worded_id_x)\n",
    "x_test_word = get_ngramed_id_x(x_test, worded_id_x)\n",
    "\n",
    "x_train_char_seq = get_ngramed_id_x(x_train, chared_id_x)\n",
    "x_test_char_seq = get_ngramed_id_x(x_test, chared_id_x)\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_dev_step(x, y, emb_mode, is_train=True):\n",
    "    if is_train: \n",
    "        p = 0.5\n",
    "    else: \n",
    "        p = 1.0\n",
    "    if emb_mode == 1: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_char_seq: x[0],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}  \n",
    "    elif emb_mode == 2: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_word: x[0],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    elif emb_mode == 3: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_char_seq: x[0],\n",
    "            cnn.input_x_word: x[1],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    elif emb_mode == 4: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_word: x[0],\n",
    "            cnn.input_x_char: x[1],\n",
    "            cnn.input_x_char_pad_idx: x[2],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    elif emb_mode == 5:  \n",
    "        feed_dict = {\n",
    "            cnn.input_x_char_seq: x[0],\n",
    "            cnn.input_x_word: x[1],\n",
    "            cnn.input_x_char: x[2],\n",
    "            cnn.input_x_char_pad_idx: x[3],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    if is_train:\n",
    "        _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "    else: \n",
    "        step, loss, acc = sess.run([global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "    return step, loss, acc\n",
    "\n",
    "def make_batches(x_train_char_seq, x_train_word, x_train_char, y_train, batch_size, nb_epochs, shuffle=False):\n",
    "    if config[\"model\"][\"emb_mode\"] == 1:  \n",
    "        batch_data = list(zip(x_train_char_seq, y_train))\n",
    "    elif config[\"model\"][\"emb_mode\"] == 2:  \n",
    "        batch_data = list(zip(x_train_word, y_train))\n",
    "    elif config[\"model\"][\"emb_mode\"] == 3:  \n",
    "        batch_data = list(zip(x_train_char_seq, x_train_word, y_train))\n",
    "    elif config[\"model\"][\"emb_mode\"] == 4:\n",
    "         batch_data = list(zip(x_train_char, x_train_word, y_train))\n",
    "    elif config[\"model\"][\"emb_mode\"] == 5:  \n",
    "        batch_data = list(zip(x_train_char, x_train_word, x_train_char_seq, y_train))\n",
    "    batches = batch_iter(batch_data, batch_size, nb_epochs, shuffle)\n",
    "\n",
    "    if nb_epochs > 1: \n",
    "        nb_batches_per_epoch = int(len(batch_data)/batch_size)\n",
    "        if len(batch_data)%batch_size != 0:\n",
    "            nb_batches_per_epoch += 1\n",
    "        nb_batches = int(nb_batches_per_epoch * nb_epochs)\n",
    "        return batches, nb_batches_per_epoch, nb_batches\n",
    "    else:\n",
    "        return batches \n",
    "\n",
    "def prep_batches(batch):\n",
    "    if config[\"model\"][\"emb_mode\"] == 1:\n",
    "        x_char_seq, y_batch = zip(*batch)\n",
    "    elif config[\"model\"][\"emb_mode\"] == 2:\n",
    "        x_word, y_batch = zip(*batch)\n",
    "    elif config[\"model\"][\"emb_mode\"] == 3:\n",
    "        x_char_seq, x_word, y_batch = zip(*batch)\n",
    "    elif config[\"model\"][\"emb_mode\"] == 4:\n",
    "        x_char, x_word, y_batch = zip(*batch)\n",
    "    elif config[\"model\"][\"emb_mode\"] == 5:\n",
    "        x_char, x_word, x_char_seq, y_batch = zip(*batch)\n",
    "\n",
    "    x_batch = []\n",
    "    if config[\"model\"][\"emb_mode\"] in [1, 3, 5]:\n",
    "        x_char_seq = pad_seq_in_word(x_char_seq, config[\"data\"][\"max_len_chars\"])\n",
    "        x_batch.append(x_char_seq)\n",
    "    if config[\"model\"][\"emb_mode\"] in [2, 3, 4, 5]:\n",
    "        x_word = pad_seq_in_word(x_word, config[\"data\"][\"max_len_words\"])\n",
    "        x_batch.append(x_word)\n",
    "    if config[\"model\"][\"emb_mode\"] in [4, 5]:\n",
    "        x_char, x_char_pad_idx = pad_seq(x_char, config[\"data\"][\"max_len_words\"], config[\"data\"][\"max_len_subwords\"], config[\"model\"][\"emb_dim\"])\n",
    "        x_batch.extend([x_char, x_char_pad_idx])\n",
    "    return x_batch, y_batch\n",
    "\n",
    "# TensorFlow Session Setup\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            char_ngram_vocab_size=len(ngrams_dict) + 1,\n",
    "            word_ngram_vocab_size=len(words_dict) + 1,\n",
    "            char_vocab_size=len(chars_dict) + 1,\n",
    "            embedding_size=config[\"model\"][\"emb_dim\"],\n",
    "            word_seq_len=config[\"data\"][\"max_len_words\"],\n",
    "            char_seq_len=config[\"data\"][\"max_len_chars\"],\n",
    "            l2_reg_lambda=config[\"train\"][\"l2_reg_lambda\"],\n",
    "            mode=config[\"model\"][\"emb_mode\"],\n",
    "            filter_sizes=list(map(int, config[\"model\"][\"filter_sizes\"].split(\",\")))\n",
    "        )\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(config[\"train\"][\"lr\"])\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        print(\"Writing to {}\\n\".format(config[\"log\"][\"output_dir\"]))\n",
    "        if not os.path.exists(config[\"log\"][\"output_dir\"]):\n",
    "            os.makedirs(config[\"log\"][\"output_dir\"])\n",
    "\n",
    "        # Save dictionary files\n",
    "        ngrams_dict_dir = config[\"log\"][\"output_dir\"] + \"subwords_dict.p\"\n",
    "        pickle.dump(ngrams_dict, open(ngrams_dict_dir, \"wb\"))\n",
    "        words_dict_dir = config[\"log\"][\"output_dir\"] + \"words_dict.p\"\n",
    "        pickle.dump(words_dict, open(words_dict_dir, \"wb\"))\n",
    "        chars_dict_dir = config[\"log\"][\"output_dir\"] + \"chars_dict.p\"\n",
    "        pickle.dump(chars_dict, open(chars_dict_dir, \"wb\"))\n",
    "\n",
    "        # Save training and validation logs\n",
    "        train_log_dir = config[\"log\"][\"output_dir\"] + \"train_logs.csv\"\n",
    "        with open(train_log_dir, \"w\") as f:\n",
    "            f.write(\"step,time,loss,acc\\n\")\n",
    "        val_log_dir = config[\"log\"][\"output_dir\"] + \"val_logs.csv\"\n",
    "        with open(val_log_dir, \"w\") as f:\n",
    "            f.write(\"step,time,loss,acc\\n\")\n",
    "\n",
    "        # Save model checkpoints\n",
    "        checkpoint_dir = config[\"log\"][\"output_dir\"] + \"checkpoints/\"\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        checkpoint_prefix = checkpoint_dir + \"model\"\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Prepare batches\n",
    "        train_batches, nb_batches_per_epoch, nb_batches = make_batches(x_train_char_seq, x_train_word, x_train_char, y_train,\n",
    "                                                                      config[\"train\"][\"batch_size\"],\n",
    "                                                                      config['train']['nb_epochs'], True)\n",
    "\n",
    "        min_dev_loss = float('Inf')\n",
    "        dev_loss = float('Inf')\n",
    "        dev_acc = 0.0\n",
    "        print(\"Number of batches in total: {}\".format(nb_batches))\n",
    "        print(\"Number of batches per epoch: {}\".format(nb_batches_per_epoch))\n",
    "\n",
    "        # Training loop\n",
    "        it = tqdm(range(nb_batches), desc=\"emb_mode {} delimit_mode {} train_size {}\".format(\n",
    "            config[\"model\"][\"emb_mode\"], config[\"data\"][\"delimit_mode\"], x_train.shape[0]), ncols=0)\n",
    "        for idx in it:\n",
    "            batch = next(train_batches)\n",
    "            x_batch, y_batch = prep_batches(batch)\n",
    "            step, loss, acc = train_dev_step(x_batch, y_batch, emb_mode=config[\"model\"][\"emb_mode\"], is_train=True)\n",
    "            if step % config[\"log\"][\"print_every\"] == 0:\n",
    "                with open(train_log_dir, \"a\") as f:\n",
    "                    f.write(\"{:d},{:s},{:e},{:e}\\n\".format(step, datetime.datetime.now().isoformat(), loss, acc))\n",
    "                it.set_postfix(\n",
    "                    trn_loss='{:.3e}'.format(loss),\n",
    "                    trn_acc='{:.3e}'.format(acc),\n",
    "                    dev_loss='{:.3e}'.format(dev_loss),\n",
    "                    dev_acc='{:.3e}'.format(dev_acc),\n",
    "                    min_dev_loss='{:.3e}'.format(min_dev_loss))\n",
    "            if step % config[\"log\"][\"eval_every\"] == 0 or idx == (nb_batches - 1):\n",
    "                total_loss = 0\n",
    "                nb_corrects = 0\n",
    "                nb_instances = 0\n",
    "                test_batches = make_batches(x_test_char_seq, x_test_word, x_test_char, y_test,\n",
    "                                            config['train']['batch_size'], 1, False)\n",
    "                for test_batch in test_batches:\n",
    "                    x_test_batch, y_test_batch = prep_batches(test_batch)\n",
    "                    step, batch_dev_loss, batch_dev_acc = train_dev_step(x_test_batch, y_test_batch,\n",
    "                                                                         emb_mode=config[\"model\"][\"emb_mode\"],\n",
    "                                                                         is_train=False)\n",
    "                    nb_instances += x_test_batch[0].shape[0]\n",
    "                    total_loss += batch_dev_loss * x_test_batch[0].shape[0]\n",
    "                    nb_corrects += batch_dev_acc * x_test_batch[0].shape[0]\n",
    "\n",
    "                dev_loss = total_loss / nb_instances\n",
    "                dev_acc = nb_corrects / nb_instances\n",
    "                with open(val_log_dir, \"a\") as f:\n",
    "                    f.write(\"{:d},{:s},{:e},{:e}\\n\".format(step, datetime.datetime.now().isoformat(), dev_loss, dev_acc))\n",
    "                if step % config[\"log\"][\"checkpoint_every\"] == 0 or idx == (nb_batches - 1):\n",
    "                    if dev_loss < min_dev_loss:\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=step)\n",
    "                        min_dev_loss = dev_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27c269c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished build vocabulary and mapping to x in 0.09513211250305176\n",
      "Size of word vocabulary: 9601\n",
      "Size of subword vocabulary (train): 82\n",
      "Size of word vocabulary (train): 5588\n",
      "Index of <UNKNOWN> word: 1182\n",
      "Processing url #0\n",
      "Number of testing urls: 7137\n",
      "INFO:tensorflow:Restoring parameters from runs/10000/checkpoints/model-2245\n",
      "Number of batches in total: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emb_mode 5 delimit_mode 1 test_size 7137: 100% 56/56 [00:16<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 2508\n",
      "False Positives (FP): 109\n",
      "False Negatives (FN): 199\n",
      "True Negatives (TN): 4321\n",
      "Accuracy: 0.9568446125823175\n",
      "Precision: 0.9583492548719909\n",
      "Recall: 0.9264868858514961\n",
      "F1 Score: 0.9421487603305785\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-56b34d113f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;31m# Save test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m \u001b[0msave_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/PhishingBaseline/URLNet/utils.py\u001b[0m in \u001b[0;36msave_test_result\u001b[0;34m(urls, labels, all_predictions, all_scores, output_dir)\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0moutput_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0msoftmax_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_scores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"url\\tlabel\\tpredict\\tscore\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../data/'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow.compat.v1 as tf\n",
    "from utils import *\n",
    "import h5py\n",
    "\n",
    "# Disable TensorFlow 2 behaviors\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'data': {\n",
    "        'max_len_words': 200,\n",
    "        'max_len_chars': 200,\n",
    "        'max_len_subwords': 20,\n",
    "        'data_dir': 'path/to/data',\n",
    "        'delimit_mode': 1,\n",
    "        'subword_dict_dir': 'runs/10000/subwords_dict.p',  # Directory of subword dictionary\n",
    "        'word_dict_dir': 'runs/10000/words_dict.p',  # Directory of word dictionary\n",
    "        'char_dict_dir': 'runs/10000/chars_dict.p',  # Directory of character dictionary\n",
    "    },\n",
    "    'model': {\n",
    "        'emb_dim': 32,\n",
    "        'emb_mode': 5\n",
    "    },\n",
    "    'test': {\n",
    "        'batch_size': 128\n",
    "    },\n",
    "    'log': {\n",
    "        'output_dir': '../data/',\n",
    "        'checkpoint_dir': 'runs/10000/checkpoints/model-2245'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load data\n",
    "with h5py.File('../data/phishing_output.h5', 'r') as h5_file:\n",
    "    # Pull 'urls' and 'labels' from the 'producthunt' dataset\n",
    "    urls = [url.decode('utf-8') for url in h5_file['test/urls'][:]]\n",
    "    labels = h5_file['test/labels'][:]\n",
    "    \n",
    "\n",
    "# Prepare data\n",
    "x, word_reverse_dict = get_word_vocab(urls, config['data']['max_len_words'])\n",
    "word_x = get_words(x, word_reverse_dict, config['data']['delimit_mode'], urls)\n",
    "\n",
    "ngram_dict = pickle.load(open(config['data']['subword_dict_dir'], \"rb\"))\n",
    "print(\"Size of subword vocabulary (train): {}\".format(len(ngram_dict)))\n",
    "word_dict = pickle.load(open(config['data']['word_dict_dir'], \"rb\"))\n",
    "print(\"Size of word vocabulary (train): {}\".format(len(word_dict)))\n",
    "ngramed_id_x, worded_id_x = ngram_id_x_from_dict(word_x, config['data']['max_len_subwords'], ngram_dict, word_dict)\n",
    "chars_dict = pickle.load(open(config['data']['char_dict_dir'], \"rb\"))\n",
    "chared_id_x = char_id_x(urls, chars_dict, config['data']['max_len_chars'])\n",
    "\n",
    "print(\"Number of testing urls: {}\".format(len(labels)))\n",
    "\n",
    "# Evaluation function\n",
    "def test_step(x, emb_mode):\n",
    "    p = 1.0\n",
    "    if emb_mode == 1:\n",
    "        feed_dict = {\n",
    "            input_x_char_seq: x[0],\n",
    "            dropout_keep_prob: p\n",
    "        }\n",
    "    elif emb_mode == 2:\n",
    "        feed_dict = {\n",
    "            input_x_word: x[0],\n",
    "            dropout_keep_prob: p\n",
    "        }\n",
    "    elif emb_mode == 3:\n",
    "        feed_dict = {\n",
    "            input_x_char_seq: x[0],\n",
    "            input_x_word: x[1],\n",
    "            dropout_keep_prob: p\n",
    "        }\n",
    "    elif emb_mode == 4:\n",
    "        feed_dict = {\n",
    "            input_x_word: x[0],\n",
    "            input_x_char: x[1],\n",
    "            input_x_char_pad_idx: x[2],\n",
    "            dropout_keep_prob: p\n",
    "        }\n",
    "    elif emb_mode == 5:\n",
    "        feed_dict = {\n",
    "            input_x_char_seq: x[0],\n",
    "            input_x_word: x[1],\n",
    "            input_x_char: x[2],\n",
    "            input_x_char_pad_idx: x[3],\n",
    "            dropout_keep_prob: p\n",
    "        }\n",
    "    preds, s = sess.run([predictions, scores], feed_dict)\n",
    "    return preds, s\n",
    "\n",
    "# Load model and run evaluation\n",
    "checkpoint_file = config['log']['checkpoint_dir']\n",
    "total_time = 0\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        if config['model']['emb_mode'] in [1, 3, 5]:\n",
    "            input_x_char_seq = graph.get_operation_by_name(\"input_x_char_seq\").outputs[0]\n",
    "        if config['model']['emb_mode'] in [2, 3, 4, 5]:\n",
    "            input_x_word = graph.get_operation_by_name(\"input_x_word\").outputs[0]\n",
    "        if config['model']['emb_mode'] in [4, 5]:\n",
    "            input_x_char = graph.get_operation_by_name(\"input_x_char\").outputs[0]\n",
    "            input_x_char_pad_idx = graph.get_operation_by_name(\"input_x_char_pad_idx\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        scores = graph.get_operation_by_name(\"output/scores\").outputs[0]\n",
    "\n",
    "        # Create batches\n",
    "        if config['model']['emb_mode'] == 1:\n",
    "            batches = batch_iter(list(chared_id_x), config['test']['batch_size'], 1, shuffle=False)\n",
    "        elif config['model']['emb_mode'] == 2:\n",
    "            batches = batch_iter(list(worded_id_x), config['test']['batch_size'], 1, shuffle=False)\n",
    "        elif config['model']['emb_mode'] == 3:\n",
    "            batches = batch_iter(list(zip(chared_id_x, worded_id_x)), config['test']['batch_size'], 1, shuffle=False)\n",
    "        elif config['model']['emb_mode'] == 4:\n",
    "            batches = batch_iter(list(zip(ngramed_id_x, worded_id_x)), config['test']['batch_size'], 1, shuffle=False)\n",
    "        elif config['model']['emb_mode'] == 5:\n",
    "            batches = batch_iter(list(zip(ngramed_id_x, worded_id_x, chared_id_x)), config['test']['batch_size'], 1, shuffle=False)\n",
    "\n",
    "        all_predictions = []\n",
    "        all_scores = []\n",
    "\n",
    "        nb_batches = int(len(labels) / config['test']['batch_size'])\n",
    "        if len(labels) % config['test']['batch_size'] != 0:\n",
    "            nb_batches += 1\n",
    "        print(\"Number of batches in total: {}\".format(nb_batches))\n",
    "        it = tqdm(range(nb_batches), desc=\"emb_mode {} delimit_mode {} test_size {}\".format(config['model']['emb_mode'], config['data']['delimit_mode'], len(labels)), ncols=0)\n",
    "        for idx in it:\n",
    "            batch = next(batches)\n",
    "\n",
    "            if config['model']['emb_mode'] == 1:\n",
    "                x_char_seq = batch\n",
    "            elif config['model']['emb_mode'] == 2:\n",
    "                x_word = batch\n",
    "            elif config['model']['emb_mode'] == 3:\n",
    "                x_char_seq, x_word = zip(*batch)\n",
    "            elif config['model']['emb_mode'] == 4:\n",
    "                x_char, x_word = zip(*batch)\n",
    "            elif config['model']['emb_mode'] == 5:\n",
    "                x_char, x_word, x_char_seq = zip(*batch)\n",
    "\n",
    "            x_batch = []\n",
    "            if config['model']['emb_mode'] in [1, 3, 5]:\n",
    "                x_char_seq = pad_seq_in_word(x_char_seq, config['data']['max_len_chars'])\n",
    "                x_batch.append(x_char_seq)\n",
    "            if config['model']['emb_mode'] in [2, 3, 4, 5]:\n",
    "                x_word = pad_seq_in_word(x_word, config['data']['max_len_words'])\n",
    "                x_batch.append(x_word)\n",
    "            if config['model']['emb_mode'] in [4, 5]:\n",
    "                x_char, x_char_pad_idx = pad_seq(x_char, config['data']['max_len_words'], config['data']['max_len_subwords'], config['model']['emb_dim'])\n",
    "                x_batch.extend([x_char, x_char_pad_idx])\n",
    "\n",
    "            start_time = time.time()\n",
    "            batch_predictions, batch_scores = test_step(x_batch, config['model']['emb_mode'])\n",
    "            total_time += time.time() - start_time\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "            all_scores.extend(batch_scores)\n",
    "\n",
    "            it.set_postfix()\n",
    "\n",
    "# Calculate accuracy\n",
    "if labels is not None:\n",
    "    # Assuming all_predictions and labels are NumPy arrays with binary values (0 or 1)\n",
    "    TP = np.sum((all_predictions == 1) & (labels == 1))  # True Positives\n",
    "    FP = np.sum((all_predictions == 1) & (labels == 0))  # False Positives\n",
    "    FN = np.sum((all_predictions == 0) & (labels == 1))  # False Negatives\n",
    "    TN = np.sum((all_predictions == 0) & (labels == 0))  # True Negatives\n",
    "\n",
    "    # Accuracy\n",
    "    correct_preds = TP + TN\n",
    "    accuracy = correct_preds / float(len(labels))\n",
    "\n",
    "    # Precision, Recall, and F1 Score\n",
    "    precision = TP / float(TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / float(TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Output all the counts and metrics\n",
    "    print(f\"True Positives (TP): {TP}\")\n",
    "    print(f\"False Positives (FP): {FP}\")\n",
    "    print(f\"False Negatives (FN): {FN}\")\n",
    "    print(f\"True Negatives (TN): {TN}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "# Save test results\n",
    "save_test_result(urls, labels, all_predictions, all_scores, config['log']['output_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bf26eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false positives: 109\n",
      "Number of false negatives: 199\n",
      "False Positives:  ['https://romeo-template.webflow.io/', 'https://ogc.irmau.com/site/content/', 'https://sekaimura.base.shop/', 'https://tigerdirect.com/td/td-sunset.html', 'https://www.plaync.com/support/500', 'https://link-in-bio.webflow.io/', 'https://kinobar.vip/', 'http://siap-online.com/', 'http://fantajia35.seesaa.net/', 'https://meumomentodevida.libertyseguros.com.br/', 'https://skwpspace.com/', 'https://mtcg.glitch.me/', 'https://allpropavingasphalt.com/', 'http://ferien.nessmersiel-nordsee.com/', 'http://www.all-nationz.com/', 'https://ascrew.shop/', 'http://mapabc.wikidot.com/', 'https://webflow-stripe-shopify.webflow.io/', 'https://sapporo-kataduke-bz.com/', 'https://benefits.proskauerpodcasts.com/', 'https://cruize-v.webflow.io/', 'https://h-o-g-w-a-r-t-s.com/', 'https://en.smarttools.xyz/', 'http://pinkfloyd.cocolog-nifty.com/', 'http://streamlinewindows.com.au/index.html', 'https://lamp-app.surge.sh/', 'https://risk.lexisnexis.com/products/behaviosec', 'https://event.insuranceinsider.com/', 'https://crx7601.com/', 'https://webofscience.help.clarivate.com/', 'https://ga-dev-tools.web.app/', 'https://freepps.top/', 'https://mangadex.org/group/4b11e95f-a9c2-417d-a179-d6a20255b68d/mangazuki', 'https://ama.blueconic.net/blueconic/static/index.html', 'https://florist.flowershopnetwork.com/websites', 'https://postupi.online/', 'http://matomecup.com/', 'https://alan-stivell.discuforum.info/index.php', 'https://rejseplanen.dk/webapp/', 'http://mindhack2ch.com/', 'http://darksouls.wikidot.com/', 'http://orcz.com/Main_Page', 'https://dullenkopf-seo-agentur-biberach.blogspot.com/', 'https://3156.cn/', 'https://www.anthropologie.com/bhldn-weddings', 'https://mangools.com/kwfinder/', 'https://bancobcr.com/wps/portal/bcr', 'https://manatelugumovies.cc/', 'https://halykbank.kz/cards/amex', 'https://bandainamco-am.com/ecommerce/', 'http://digital-thread.com/', 'https://forzamigliozzi.com/', 'https://97luhi.click/', 'http://www.sexwebvideo.com/', 'http://www.saikyo-jump.com/', 'https://ixdzs8.com/', 'https://afflift.com/f/threads/i-bought-peerfly-com.12297/', 'https://services.bahrain.bh/wps/portal/ar/BSP/HomeeServicesPortal/', 'http://www.1010jiajiao.com/', 'https://delivery-club.ru/showcaptcha', 'http://globaljusticeecology.org/our-no-ge-trees-program/', 'https://yummyyummyyummy.blogspot.com/', 'https://gtdev-test.shopify.gt.ht/', 'https://voicewave.xyz/', 'https://beckershealthcare.uberflip.com/read/account_titles/193377', 'https://sistema.welivery.com.ar/site/login', 'https://coco885bank885.pixnet.net/blog', 'http://www.jogosgratispro.com/', 'https://johnnytzgl18417.myparisblog.com/', 'https://bigin.netlify.app/', 'https://vettemplates.webflow.io/', 'https://ideas.exlibrisgroup.com/', 'https://ff3456.com/', 'https://catlak-site55.tr.gg/', 'https://sq-al.facebook.com/login/', 'https://shannonmulti.tempurl.host/wp-signup.php', 'https://100days100grand.com/', 'https://ok-ki-fuer-zukunftsoptimisten.podigee.io/', 'http://sergeidovlatov.com/', 'https://uzox.xyz/', 'https://auburn1856.com/', 'https://k-clinic-nanase.com/', 'https://resources.businesstalentgroup.com/', 'https://luatnhandanvn.mystrikingly.com/', 'https://tsakhkadzor-kotayk.am/Pages/Home/Default.aspx', 'https://hello1103.official.ec/', 'https://mailchi.mp/a5d0b3956e86/jaylarkstudios', 'https://docs.friendly.ch/da9b70a93b5544629d7c6dce11f6ea17', 'https://morrisonglobal.square.site/', 'https://socialgood-whitepaper.com/', 'https://template-metric.webflow.io/', 'https://poppins-on-mackinac.myshopify.com/', 'https://web32.xyz/', 'https://live-ucb-cej.pantheonsite.io/', 'https://countdown-landing.webflow.io/', 'https://localadventure.webflow.io/', 'https://blimps.xyz/', 'https://passwordprotectedwp.com/', 'http://www.praha-levne.cz/', 'https://www.otto-schmidt.de/zeitschriften/wirtschaftsrecht/zip-zeitschrift-fur-wirtschaftsrecht-probeabo-07239416', 'http://nipponbaseball.seesaa.net/', 'https://serviceability-dev.openjdk.java.narkive.com/', 'https://bassifamily.com/Live/', 'https://portal.tufin.com/s/', 'https://elzasyati45.blogspot.com/', 'https://ifx24.com/', 'https://lawrenceks.civicweb.net/portal/', 'https://337799.com/', 'https://florida-animation-festival.square.site/']\n",
      "False Negatives:  ['https://transfercargousa.com/', 'https://sneakerdrawp.com/', 'https://followparcel-ups.com/', 'https://vpn.maisir.link/', 'https://wodownbg.com/', 'https://djinnglory.net/', 'https://imaeepp.com/', 'https://syncportalbrowser.com/', 'https://mj-api.sopenai.com/', 'https://www.cognitoforms.com/FiRe21/EmployeeTeammateQuarterNominationPeriodForMidyear', 'https://www.it-admingroup.com/', 'https://free.yemo.eu.org/', 'https://macartevitale-assu.info/', 'https://xejdwle.gruposabar.com/', 'https://ggboy2794.free.hr/', 'https://clubsidedev.com/', 'https://teiegam.us/', 'https://doculuma.com/', 'https://uzielchavo2005.com/', 'https://imap.troyriser.com/', 'https://sanmati.inspiresoftware.co.in/', 'https://www.dbsingapore.com/', 'https://elrehabholidays.com/', 'https://www.mutabusinne.com/arapt2', 'https://danskoco.com/', 'https://cybear-shock.ru/', 'http://www.deblizzard.net/', 'https://merkeznoktadai.net/', 'https://www.micozzimotostoriche.it/track/dechu/Verifiera_din_leveransadress/', 'https://dev.casaderincon.com/', 'https://tbk.tm/', 'https://d.soderbergpartners.no/', 'https://airdrop.manta-network.de/', 'https://wealthcycle.club/', 'https://whats-ws.org/', 'https://jantanslot88ku.com/', 'https://skype-in.net/', 'https://marhenzproperties.com/', 'https://flirtysites.com/', 'https://aidanochka-tabys.kz/', 'http://www1.frameupstudios.com/', 'https://www.storage-limit.com/', 'https://4betpromotion.blogspot.com/', 'https://gruposolutech.com.br/', 'https://live-login.com-encrypted.net/en/', 'https://beutopiantech.com/-/', 'https://hrklm.pw/', 'https://trezo-start.com/', 'https://rpcnodesworldwide.com/', 'https://portal.hmax.com.br/', 'https://dev.informed.uspsinnovation.tech/', 'https://www.meoforkids.com/', 'https://itsupports.ie/', 'https://alda9049.hocoos.com/', 'http://www.yusung.com.cn/', 'https://urlsrt.io/LhDeB', 'https://fledgelingsnurseryschool.co.uk/', 'https://www.reelymm.com/chn/index.php', 'https://hrmf5.com/', 'https://suspension-sg.info/', 'https://tradebot.tenzormatic.com/', 'https://omg.ciroc.pp.ua/', 'https://shop.feinsauber.de/new1/', 'https://flaretrustline-evm.net/', 'https://apple.crashcop.com/', 'https://www.shenyangpr.com/', 'https://companyoperation.com/', 'https://mjahanbani.com/', 'https://dr-martens-boots.ru/', 'http://enduro.siciliaenduro.it/', 'https://exodusgroup.io/', 'https://biglobe.evencarellc.com/', 'https://www.hopp.co/blocked', 'https://constructorawerner.cl/', 'https://www.mercadoilbre.com/', 'https://todoenseries.com/', 'https://www.transparency-business.com/', 'https://www.haberler-yeni-com.satilir.com.tr/', 'https://ggkjgcb.com/', 'https://goriladabet.com.br/dan/', 'https://ecometanexus.unids.com/', 'https://adrianatg.com/', 'https://disumin.com/', 'https://artglidermail.com/', 'https://whats-io.com/', 'https://coachingleverage.com/', 'https://hanlogistics.info.gf/', 'https://contenttriviainterfrindlyt.blogspot.com/', 'https://www.praiagrande.site/', 'https://teiegramg.org/', 'https://www.jpm-secure.com/user/register', 'https://airbnbreplicate.ritaguilherme.com/', 'https://clicker.extremelyorange.com/', 'https://radio93fm.com/', 'https://egc603.com/', 'https://ebay201.com/', 'https://shoutout.wix.com/so/e1P6lraVU', 'https://bywzygy.com/', 'https://apuniswap.com/', 'https://xgames.midasbuyspro.com/', 'https://tri-level.net/', 'https://acgmw.com/', 'https://enonline.mcleanchasecondos.com/', 'https://stampstops.com/', 'https://spins.midasbuyspro.com/', 'https://uspsdeliveries.com/', 'https://validaite.net/', 'https://support-help.co.uk/', 'https://join.eventnowz.com/', 'https://games.midasbuyspro.com/', 'https://teieg-arm.com/', 'https://battle.enentmax.com/', 'https://allegroexpress.ru/', 'https://encrypted-portal.com/', 'https://schwabachprotection.com/', 'https://anabeldib.ru/', 'https://www.portal-de-recargas.com/', 'https://arksmart.net/MT/', 'https://bsxshy.com/', 'https://1inlirh-oi.org/', 'https://mfbnt.com/', 'https://www.ynmlife.com/', 'https://zereons.com/', 'https://glfcfs.com/', 'https://manully-migratedapp.com/', 'https://sestemfor9a.blogspot.com/', 'http://www.cora.cyou/', 'https://ghazalischools.com/youtube/', 'https://banyuslud.ru/keee/pasword', 'https://administracion.babelviajes.com/', 'https://www.tkmerchant.com/', 'https://uplfitingclubs.com/', 'https://jantanslot88kilat.com/', 'https://dieu-huyen.semat.cl/', 'https://chat.ugbumr.cn/', 'https://or-ins.org/', 'https://ingresosorteaos.lat/', 'https://lamiaaziendaonline.com/', 'https://banaes.com/', 'https://upsprioritymails.com/', 'https://app.consultorioonline.org/alcance', 'https://www.dshkbf.com/', 'https://www.oo1ll740ed.com/', 'https://locality.microsoft-alerts.net/', 'https://www.comprarnamagalu.com.br/', 'https://xxxx.amberwritings.com/', 'https://guilinlishi.com/', 'https://onlineaktrweb.com/', 'https://save2.cechire.com/', 'https://pubsec.agreementsdemo.com/', 'https://match.lookatmynewphotos.com/', 'https://binanceeuro.com/', 'https://shopeecoins.blogspot.com/', 'https://vixviqoi.com/', 'https://denemed.com/ch/', 'https://newoneath.ubpages.com/openmlol/', 'https://esiteonlin.net/', 'https://tkwholesalepro.com/', 'https://www.365outlook.net/', 'https://xiaosheng8.com/', 'https://landxt.com/', 'http://catchyourluck.agency/', 'https://recoverme.nz/', 'https://trwebmobilak.com/', 'https://buildkartindia.com/', 'https://snetforyou.blogspot.com/', 'https://ausreportinbox.com/', 'https://aceng.akuanakmamah.de/', 'https://www.onnlibnd24.com/', 'https://www.poiujj.dns-dynamic.net/', 'https://booking-keys-sub.baby/', 'https://munasur-udla.webnode.es/carta-del-munasur-udla/', 'https://www.studyinblcu.net/', 'https://www.drmartensencolombia.com.co/', 'https://www.achillebellomi.it/fire.html', 'https://www.e-messsage.com/', 'https://pizala.ru/', 'https://berlineditz.blogspot.com/', 'https://nfcu-regain1.com/', 'https://jtsgoods.com/', 'https://maintenance.lotteryoffice.com/', 'https://refund-sars.com/', 'https://aldclub.com/', 'https://bn54.donegabang.com/', 'https://ridgemarine.com/', 'https://henggugroup.com/', 'https://dapadar.com/', 'https://win-prizes.live/', 'https://hpanel.hostinger.com/', 'https://www.indicadores.siciudadania.co/', 'https://nl-rober-tdecriptop.com/', 'https://weeblyunit.com/', 'https://imktoklen.com/en.html', 'https://tk-eale24h.com/', 'https://ofertas-especiales.info/', 'https://page.voluntarycensorshippolicy.com/', 'https://stockcenter-ar.com/', 'https://sparkvm1.com/', 'https://mail-sgh.waw.pl/']\n"
     ]
    }
   ],
   "source": [
    "false_positives = [urls[i] for i in range(len(labels)) if all_predictions[i] == 1 and labels[i] == 0]\n",
    "false_negatives = [urls[i] for i in range(len(labels)) if all_predictions[i] == 0 and labels[i] == 1]\n",
    "\n",
    "print(\"Number of false positives: {}\".format(len(false_positives)))\n",
    "print(\"Number of false negatives: {}\".format(len(false_negatives)))\n",
    "print(\"False Positives: \", false_positives)\n",
    "print(\"False Negatives: \", false_negatives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45517198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd68d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phish-base)",
   "language": "python",
   "name": "phish-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
